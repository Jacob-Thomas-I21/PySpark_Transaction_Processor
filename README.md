# PySpark Transaction Processor

A high-performance transaction processing system that ingests data from Google Drive and detects customer upgrade patterns using PySpark. Designed to handle 10,000+ transactions per second with real-time pattern detection capabilities.

## ðŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Google Drive  â”‚â”€â”€â”€â–¶â”‚   Mechanism X   â”‚â”€â”€â”€â–¶â”‚      AWS S3     â”‚
â”‚  (CSV Files)    â”‚    â”‚  (Data Ingest)  â”‚    â”‚ (Raw Storage)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚â—€â”€â”€â”€â”‚   Mechanism Y   â”‚â—€â”€â”€â”€â”‚   PySpark       â”‚
â”‚   (Results)     â”‚    â”‚ (Pattern Detect)â”‚    â”‚  (Processing)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“Š Data Flow Overview

### **Phase 1: Data Ingestion (Mechanism X)**
1. **Google Drive Monitoring**: Continuously monitors specified folder for new CSV files
2. **File Processing**: Downloads and validates transaction files (10K+ records/sec capability)
3. **S3 Upload**: Streams processed data to AWS S3 with partitioned structure
4. **Metadata Tracking**: Records file processing status in PostgreSQL

### **Phase 2: Pattern Detection (Mechanism Y)**
1. **S3 Data Reading**: PySpark reads partitioned data from S3 buckets
2. **Data Transformation**: Applies schema validation and data cleaning
3. **Pattern Analysis**: Uses Spark Window functions for percentile calculations
4. **Result Storage**: Saves detected patterns to PostgreSQL and S3

## ðŸŽ¯ Pattern Detection Logic

### **PatId1 - Customer Upgrade Detection**
Identifies potential upgrade candidates using advanced analytics:

```sql
-- Customers in TOP 10% by transaction count
-- AND BOTTOM 10% by importance weight
-- For merchants with >50K total transactions
```

**Algorithm Steps:**
1. **Merchant Filtering**: Filter merchants with >50,000 transactions
2. **Percentile Calculation**: Calculate 90th percentile for transaction counts
3. **Importance Scoring**: Calculate 10th percentile for customer importance
4. **Pattern Matching**: Identify customers meeting both criteria
5. **Result Validation**: Verify pattern accuracy and store results

## ðŸš€ Quick Start

### Prerequisites
- Python 3.8+ with pip
- Java 8 or 11 (for PySpark)
- PostgreSQL 12+
- AWS S3 access credentials
- Google Drive API service account

### Installation

1. **Clone and Setup Environment**:
```bash
git clone <repository-url>
cd PySpark-Transaction-Processor
pip install -r requirements.txt
```

2. **Configure Credentials**:
```bash
cp .env.template .env
# Edit .env with your actual credentials
```

3. **Database Setup**:
```bash
# Create PostgreSQL database
createdb transaction_db
psql transaction_db < setup/postgres_setup.sql
```

4. **Google Drive API Setup**:
   - Create service account in [Google Cloud Console](https://console.cloud.google.com)
   - Download JSON credentials to `config/google_credentials.json`
   - Share target Drive folder with service account email
   - Copy folder ID from Drive URL

5. **AWS S3 Configuration**:
   - Create S3 bucket for data storage
   - Configure IAM user with S3 read/write permissions
   - Add credentials to `.env` file

### Running the System

```bash
# Start the complete pipeline
python main.py

# Run individual mechanisms
python -m src.mechanism_x.data_ingestion  # Data ingestion only
python -m src.mechanism_y.stream_processor # Pattern detection only
```

## ðŸ“ Project Structure

```
â”œâ”€â”€ main.py                           # Main application entry point
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .env.template                     # Environment configuration template
â”œâ”€â”€ .github/                          # CI/CD and GitHub configurations
â”‚   â”œâ”€â”€ workflows/ci.yml             # Automated testing pipeline
â”‚   â”œâ”€â”€ SECURITY.md                  # Security policy
â”‚   â””â”€â”€ dependabot.yml               # Dependency updates
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.py                    # Application configuration
â”‚   â”œâ”€â”€ spark_config.py              # PySpark optimization settings
â”‚   â””â”€â”€ google_credentials.json      # Google Drive API credentials
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mechanism_x/                 # Data Ingestion Pipeline
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py       # Google Drive to S3 processor
â”‚   â”‚   â””â”€â”€ file_monitor.py         # File change detection
â”‚   â”œâ”€â”€ mechanism_y/                 # Pattern Detection Engine
â”‚   â”‚   â”œâ”€â”€ stream_processor.py     # Main processing logic
â”‚   â”‚   â”œâ”€â”€ pattern_detector.py     # PatId1 implementation
â”‚   â”‚   â””â”€â”€ data_validator.py       # Data quality checks
â”‚   â”œâ”€â”€ schemas/                     # Data schemas and validation
â”‚   â”‚   â””â”€â”€ transaction_schemas.py  # Spark schema definitions
â”‚   â””â”€â”€ utils/                       # Shared utilities
â”‚       â”œâ”€â”€ postgres_utils.py       # Database operations
â”‚       â”œâ”€â”€ s3_utils.py             # AWS S3 operations
â”‚       â””â”€â”€ spark_utils.py          # Spark session management
â”œâ”€â”€ setup/                           # Database setup scripts
â”‚   â”œâ”€â”€ postgres_setup.sql          # Initial database schema
â”‚   â””â”€â”€ create_missing_tables.sql   # Additional table creation
â””â”€â”€ tests/                           # Test suite
    â”œâ”€â”€ test_mechanism_x.py         # Ingestion tests
    â”œâ”€â”€ test_mechanism_y.py         # Pattern detection tests
    â””â”€â”€ test_integration.py         # End-to-end tests
```

## âš™ï¸ Configuration

### Environment Variables (.env)
```bash
# AWS Configuration
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
S3_BUCKET_NAME=your-transaction-bucket

# PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=transaction_db
POSTGRES_USER=transaction_user
POSTGRES_PASSWORD=your_password

# Google Drive Configuration
GDRIVE_FOLDER_ID=your_folder_id
GOOGLE_CREDENTIALS_PATH=config/google_credentials.json

# Processing Configuration
BATCH_SIZE=10000
MAX_WORKERS=4
PROCESSING_INTERVAL=30
```

### Spark Configuration
- **Memory**: 4GB driver, 2GB executor
- **Cores**: Auto-detected based on system
- **Serializer**: Kryo for performance
- **Checkpointing**: Enabled for fault tolerance

## ðŸ“ˆ Performance Metrics

### **Throughput Capabilities**
- **Data Ingestion**: 10,000+ transactions/second
- **Pattern Detection**: 1M+ records processed in <5 minutes
- **S3 Upload**: Parallel multipart uploads for large files
- **Database Writes**: Batch inserts with connection pooling

### **System Requirements**
- **Minimum**: 4GB RAM, 2 CPU cores
- **Recommended**: 8GB RAM, 4 CPU cores
- **Storage**: 10GB+ free space for local processing

## ðŸ“Š Output Data Structure

### **S3 Data Organization**
```
s3://your-bucket/
â”œâ”€â”€ raw-transactions/
â”‚   â”œâ”€â”€ year=2024/month=01/day=15/
â”‚   â”‚   â”œâ”€â”€ transactions_001.parquet
â”‚   â”‚   â””â”€â”€ transactions_002.parquet
â”‚   â””â”€â”€ _metadata/
â”‚       â””â”€â”€ processing_log.json
â””â”€â”€ pattern-detections/
    â”œâ”€â”€ patid1/
    â”‚   â”œâ”€â”€ year=2024/month=01/
    â”‚   â”‚   â””â”€â”€ detections.parquet
    â”‚   â””â”€â”€ summary/
    â”‚       â””â”€â”€ monthly_stats.json
    â””â”€â”€ validation/
        â””â”€â”€ quality_reports.json
```

### **PostgreSQL Tables**
- **`transactions`**: Raw transaction data with indexing
- **`customer_importance`**: Customer scoring and weights
- **`pattern_detections`**: PatId1 results with timestamps
- **`processing_logs`**: System monitoring and audit trail

## ðŸ” Monitoring & Observability

### **Logging**
- Structured JSON logging with correlation IDs
- Separate log levels for development and production
- Automatic log rotation and archival

### **Metrics**
- Processing throughput and latency tracking
- Error rates and system health monitoring
- Resource utilization (CPU, memory, disk)

### **Alerts**
- Failed file processing notifications
- Database connection issues
- S3 upload failures and retries

## ðŸ› ï¸ Development

### **Testing**
```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test categories
pytest tests/test_mechanism_x.py -v
```

### **Code Quality**
- **Linting**: Flake8 for PEP 8 compliance
- **Formatting**: Black for consistent code style
- **Security**: Bandit for security vulnerability scanning
- **Dependencies**: Safety for known security issues

### **CI/CD Pipeline**
- Automated testing on Python 3.8, 3.9, 3.10
- Security scanning and dependency checks
- Code coverage reporting
- Automated deployment to staging environment

## ðŸ”’ Security

- **Credentials**: Environment-based configuration
- **API Keys**: Encrypted storage and rotation
- **Database**: Connection encryption and user isolation
- **S3**: IAM-based access control with minimal permissions
- **Audit**: Complete processing trail and access logging

## ðŸ“ž Support

For technical issues or questions:
1. Check the [troubleshooting guide](TROUBLESHOOTING.md)
2. Review [GitHub Issues](../../issues)
3. Contact: hiring@devdolphins.com

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.